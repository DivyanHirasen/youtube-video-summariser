AMD pools are actually very very excited about this open AI and Google deal. The main reason is it kind of solidifies that the market wants more than one player in the AI chip market. But are AMD bulls correct to think this way? Let's take a closer look in today's episode. I want to thank the Mly Fool for sponsoring this video and check out fool.com/hose for the 10 best stocks to buy now. With that link, you get a promotional offer for their subscription service. Now, let's continue with today's episode. All right, so first let's take a closer look at what we know about this OpenAI and Google deal. So, first we know that Google has won OpenAI as a customer of its cloud infrastructure. More importantly, its TPUs aka its ASIC chips. Based on the reports, this is the version five and version 4 TPUs, not the newest generation, which is the Trillium 6G. And I think this is actually very telling. Now, all this has kind of created different market opinions. The first opinion, which I believe is extremely true, it is this is extremely good for Google. And I talked about it in yesterday's video. I mean, it's pretty obvious. you're getting a big AI company to join into Google Cloud's uh Google Cloud Platform. The second kind of way the market sees this is this is bad for Nvidia. And I completely disagree with this take for this news. I'm not a perma Nvidia bull. I see the competition and the competitive landscape. I just don't believe this news is bad for Nvidia. And again, I talked about it in yesterday's video. Now, is this good for AMD? And the market still hasn't really found an answer for this. And I think the answer is yes and no. And that's what I'm actually going to talk about in today's video. I also want to share some other thoughts about AMD and why I am still extremely extremely bullish at this company even though today's news might not be as bullish as some bulls make it seem out to be. The other thing is there is this massive misinformation coming out for AMD from AMD bulls themselves that I think is completely wrong. And I hope I hope I can end that misinformation in today's video. So, if this is your first time here, my name is Jose Naro. This is going to be the best place you're going to get any AI semiconductor stock market news. Not just numbers, but understanding the market. So, make sure to hit the thumbs up and the subscribe button if you want to support the channel. Also, make sure to check out some of my amazing links down below for some great great platforms. First, I kind of want to just get to know what is happening in the AI space and especially related to this news. So, I think OpenAI just needs more GPUs and you have compute needs increase from the following. First, OpenAI is getting more members on a weekly and monthly basis. So, you have compute increase from more members. The average members are using more tokens. This could be for various reasons. One more members are becoming more familiar with AI models that they use it more often. The second thing is pretty much going to now to option number three is thinking models and more advanced models use more tokens than prior models. So you have three massive massive tailwinds increasing the compute needs for open AAI. And this is crucial and leads to my following thoughts. OpenAI is not leaving Nvidia for any negative reasons, but the market is too constraint that it needs a healthy second option, which is very good for AMD and very good for ASIC. What we don't know about this OpenAI Google deal is the following. What workloads are being transferred? It is important to note that OpenAI has various types of models like large language models, imaging modules, audio models, and video models. More importantly, each type of model has different versions. Some more complex than others via thinking, some are oneshot, some are smaller in size, etc., etc. And this is crucial and leads to my following thoughts. Since we saw OpenAI is not using the most advanced TPUs, then this is OpenAI transferring less intensive workloads to TPUs, so it can use its constraint NVIDIA GPUs for the more complex task. And here I can see a subscriber of the channel C Barn explain it really well. Good video. Different workloads require different GPUs. Open AAI probably can't wait for Blackwell since it's sold out or may not need those GPUs for the models they're working on. No need to shoot a Booka out of Fly when you can use a Fly Swatter. Save the more powerful GPUs for workloads that require that performance. And I think that's exactly what's happening here. And that's what led me to my overall thoughts on yesterday's episode. Bad for Nvidia is completely the wrong take to the wrong take to have with this news. Now I want to jump into good for AMD. question mark. First I want to talk about why I do believe this is extremely good for AMD. First it shows that the market is in dire needs of more AI compute. AMD Mi300's and MI350s should have easy time selling not only from OpenAI but from other AI companies as we see it is AI compute constraint and we did see from AMD's AI event that the top seventh out of the 10 largest AI companies use AMD instinct right now. So if these are using them there is going to be a need for the MI300 the MI350s and the MI400's. The other thing that we see this is OpenAI is an avid user already of the MI300. So they could put pressure on Google Cloud to buy MI300's and MI400s. Google Cloud does not offer MI300s at the moment because they believe there's no demand for that platform. But if OpenAI can say look we'll give you even more and more demand or more kind of cloud uh customer if you open up the MI300 and the MI400 then I think Google will be more open to buy from AMD. OpenAI is also we heard from the AI platform or AI events that open AAI is helping with the design of the MI400. It also just shows that the market wants more than one option. So this is why I truly believe it is bullish for AMD. Now, I want to share why I don't think it is good for AMD. Why did OpenAI not choose AMD instead? If we're looking for the second option, why not just go to the second S source? As I am sure AMD would gladly sell more chips, could it be a timing thing? It does take time to build AMD solutions while Google Cloud Platform has readily available supply of TPUs. Still, I do believe timing is a competitive advantage in this AI space. If you had two sources and one of the sources is available to you right now, you're most likely going to pick that in this kind of race of the AI market. So, for that reason, normally it seemed that the goal for AMD is to take market share from Nvidia. But I believe this shows that the current competition for AMD is strongly developed ASIC players. One of the strengths of AMD though is that it's truly programmable and has the flexibilities that AS6 solutions typically lack. But but if AI companies just want to offload older weaker models that don't need that flexibility, then that doesn't sit well for AMD as the AS6 solution might provide a better advantage. And that's what we're seeing here right now. So that is why I believe that while this is kind of good for AMD in an overall sense of things, it does create that bearish thesis as well. So it's not completely a yes and no answer for AMD. Now before all the AMD bulls start cing hurting me for for being a little bearish on this thoughts, I do want to share some of my other thoughts on AMD. While I don't believe the overall news is truly bullish for AMD, except for the fact that the market is computed and is looking for AI companies, it is important to know that other AI companies might not want to be reliant to Google Cloud Platform if they need a second choice due to competition. Maybe Amazon, for example, wouldn't want to in extend some solutions to Google Cloud. They'll rather buy AMD's chip instead. Or they might prefer onrem. Some companies can't work on the cloud and Google cloud can't provide on-prem solutions either. So this does open up that the second opportunity is still pretty much at the opportunity for AMD. More importantly, I am extremely extremely bullish at the Mi400 which looks extremely competitive in paper to Nvidia solution and I still am a personal believer that AMD is still a multibacker from here. Now I want to talk about this extremely extremely misinformation happening with AMD. And I think this is something that I hear a lot that AMD for example again I'm not trying to shoot um send any misinformation here or trying to correct um make it seem like they were completely wrong. I think it's understandable why this misinformation is going around. Um, but in this video that I posted yesterday, someone posted, "I think you're missing AMD's place in AI." In my opinion, Nvidia will struggle to compete with them on inference, especially in 2026 when AMD releases the MI400. It's why Meta is now exclusively using AMD's GPUs over Nvidia. Now, again, I talk about it in this episode. I believe the MI400 looks extremely competitive, and I am actually very bullish for that. So, I don't think I'm missing AMD's place in AI, but I responded, "Who fed you lies that Meta is exclusively using AMD?" And NVL72 is literally a king of inference. And they responded, "Sorry." And they they kind of mentioned, "Look up AMD's advancing AI show in October of 2024." And fast forward to 51 minutes and 20, you'll hear Meta's VP of infrastructure supply and engineering say it. It is I been in AMD for a long time. I watch all these conferences and if you haven't done so, make sure to hit the thumbs up and the subscribe button as this is going to be the best place you're going to find any semiconductor news both for the bullish and bearish take. The keynote does say that Llama's 3 405 billion parameter model is exclusively on the MI300. It is important that there are to know that there are various models and many more updates since then and Meta's AI solutions is not just llama. Most of their AI solutions from Meta are actually coming from their core products like workload like recommendation system. Now luckily in the most recent AI events from AMD, Meta Plan mentioned that in addition to Llama inference, Meta's AI recommendation, inference and training models run on AMD's MI300 GPUs. So we are seeing that AMD solutions are expanding um outside of just inference. But there is one key thing here that's different. We no longer hear about any exclusivity. There are some other opinions because it is good to be bullish and I am very bullish on AMD, but bullish on wrong information can lead to overhyped expectations. And let me kind of go to the following. First, Llama 3.1 released in July 23 of 2024. That's when we saw the 405 billion parameter. Since then, Meta Platforms has released Llama 4. Now, Llama 4, we did not hear any exclusivity of the MI400 or the MI300 using of Meta using the MI300 exclusively for inference. That exclusivity was never announced again. Actually, in this paper when they released the Llama 4, you hear various in instances of Nvidia's H100 GPUs. I think about three times. None of this paper includes AMD's Mi300. Now, if we also look, there are plenty of technical blogs both from AMD and Nvidia showcasing that both of these solutions are being used um are being used for Llama 4. So again, the exclusivity seems to have been there for the 405B parameter, but it's not there for any other models, especially the most recent, the Llama 4. Another thing that I want to make uh that that I think is important to know is there is this inference benchmark that is a benchmark not controlled by one party. So it's an actual indication of who is the leader in the infant space. And if we look at here this is the llama 2 70 billion parameter model and the one providing the best inference is the MI300 with a quick caveat. This is using four nodes of AI of eight accelerators. That means that this is 32 AI accelerators generating about 103,000 tokens per second. Nvidia, the B200 is actually doing roughly the same amount with 25 with only 25% of the accelerator. So they're only using eight to kind of reach the same amount as AMD using 32. I think that is crucial especially in the market where there is power constraint. And for example, me running a server, I would rather run a server that's only using eight accelerators versus 32 as there is a lot of things that can go wrong and a lot more cooling, a lot more electricity and everything else that goes with running more nodes. Now, more importantly, they was a benchmark with the 405B parameter model, the Llama 3.1 that is exclusively run on AMD, or so it was said. Now if we look at the highest numbers, the highest number here is actually Nvidia's GB200 demolishing this massive behemoth of a model. It is also important to note that in this benchmark AMD never submitted any results. So I think that is a little bit telling of this information here that AMD is exclusively using Nvidia for inference. Again, it was only for one model and it should no longer be a thing. And this is why you need to sign up with this channel, right? Something like this could be true for a few months, but it's already about what 9 months since that event, 8 months since that event, and the market has completely shifted. So, it is very hard to keep with the same thesis and not kind of updating that thesis with new results. So hopefully this kind of ends this thought process of AMD being the exclusive inference player of Nvidia of Meta Platform. That is nowhere near true even though on the bullish side we are seeing that more recommendations and more workloads are transferring to AMD. So that is extremely bullish. So I hope you guys enjoyed today's episode. Take care. Have a good day.